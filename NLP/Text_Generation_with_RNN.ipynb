{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNSMgDAqff/oKbCLt781z+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushs0911/Projects/blob/main/NLP/Text_Generation_with_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FiBFoSTQBzIh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import os \n",
        "import time "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading dataset\n"
      ],
      "metadata": {
        "id": "RABlkCRCGdcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKQUSnYOGodH",
        "outputId": "96f221c2-2f86-4ca1-a90b-bbacc100c93f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the data"
      ],
      "metadata": {
        "id": "IK_Lws79HGmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding = 'utf-8')\n",
        "\n",
        "print(f\"Length of text : {len(text)} characters \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU-HQHADHVCB",
        "outputId": "24418522-1a59-4a03-abec-f3b01e21f8d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text : 1115394 characters \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WcYYBqN5HlWt",
        "outputId": "1388a3bb-5781-4d44-d673-2429cd456c52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check unique characters in file \n",
        "vocab = sorted(set(text))\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ9bhhNNHt8E",
        "outputId": "b0d19fdb-8e64-43c8-bf68-7c47fa714387"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Processing the text \n",
        "`tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first. "
      ],
      "metadata": {
        "id": "qaUon7vKVYvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary = list(vocab),\n",
        "    mask_token = None \n",
        ")"
      ],
      "metadata": {
        "id": "bPTPI2tOVfNR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzb67TbUV6Wd",
        "outputId": "16178932-555b-4c4c-8a22-d02fcc215e70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZnzu0coV_CF",
        "outputId": "a9ec27bf-6741-4ee9-e21e-161783c3201e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inverting this representation and recover human readable strings from it. "
      ],
      "metadata": {
        "id": "PmQcBqJEWkGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), \n",
        "    invert=True, \n",
        "    mask_token=None)"
      ],
      "metadata": {
        "id": "CTktSTcWWsRm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vDxUTmeWya-",
        "outputId": "e9e787af-a652-4235-9e37-9c3e90387933"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis = -1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf82wHzbW0MT",
        "outputId": "36caf6eb-edc2-4ef5-dc6e-831ff302ebf0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)"
      ],
      "metadata": {
        "id": "bzCpnpnnXDmb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The prediction task \n",
        "The input to the model will be a sequence of characters, and you train the model to predict the outputâ€”the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
        "\n",
        "**Create training examples and targets**\n",
        "\n",
        "- Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "- For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "- So break the text into chunks of `seq_length+1`. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "T- o do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ],
      "metadata": {
        "id": "II1KrNT9XSeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfdigINQXbyQ",
        "outputId": "cb4738e4-35da-497f-8409-1db6647e98b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "Vjja_gviX0DC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(20):\n",
        "  print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3k-Ly_HYJ2S",
        "outputId": "f168a930-0788-4643-8870-3f039fd23a6e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "z\n",
            "e\n",
            "n\n",
            ":\n",
            "\n",
            "\n",
            "B\n",
            "e\n",
            "f\n",
            "o\n",
            "r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "xgELS2wrYQrG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder = True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(seq)\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw5CgnKnYwNm",
        "outputId": "a90d21ff-aed4-42a8-c48c-a02da0db3ccc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[19 48 57 58 59  2 16 48 59 48 65 44 53 11  1 15 44 45 54 57 44  2 62 44\n",
            "  2 55 57 54 42 44 44 43  2 40 53 64  2 45 60 57 59 47 44 57  7  2 47 44\n",
            " 40 57  2 52 44  2 58 55 44 40 50  9  1  1 14 51 51 11  1 32 55 44 40 50\n",
            "  7  2 58 55 44 40 50  9  1  1 19 48 57 58 59  2 16 48 59 48 65 44 53 11\n",
            "  1 38 54 60  2], shape=(101,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwFNcYCFY68F",
        "outputId": "07d767fe-f5c4-4a04-9325-98e52d128b03"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training need dataset of `(input, label)` pairs. At each time stamp the input is the current character and label is next character. \n"
      ],
      "metadata": {
        "id": "IBx2jiXbZgzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "KjmJNVwMemQu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Ayush Singh\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwv8KEDje0KY",
        "outputId": "3ba82daf-368d-46d3-e795-20887f5d224d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['A', 'y', 'u', 's', 'h', ' ', 'S', 'i', 'n', 'g'],\n",
              " ['y', 'u', 's', 'h', ' ', 'S', 'i', 'n', 'g', 'h'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "PkBjBK4se6o_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input:\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uizrG2H1fA7B",
        "outputId": "b964369b-d096-492d-9364-99e0b8b10885"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating training batches \n",
        "Suffle the data and pack into batches. "
      ],
      "metadata": {
        "id": "lIfjwM4mfZlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (dataset\n",
        "           .shuffle(BUFFER_SIZE)\n",
        "           .batch(BATCH_SIZE, drop_remainder = True,)\n",
        "           .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXKYcCdZfksp",
        "outputId": "a04d7b80-fc26-4e12-e451-9481f25e054f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build the model\n",
        "Defines model as `keras.Model` subclass. <br>\n",
        "3 layers \n",
        "- `tf.keras.layers.Embedding`\n",
        "- `tf.keras.layers.GRU`\n",
        "- `tf.keras.layers.Dense` : output layer with `vocab_size` outputs. It outputs one logit for each character in vocabulary. "
      ],
      "metadata": {
        "id": "Q-e0yrO3f6tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lenght of vocab in StringLookup layer \n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "nfBq7ft7giWk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, \n",
        "                                   return_sequences = True, \n",
        "                                   return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training = training )\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state = states, \n",
        "                         training = training)\n",
        "    x = self.dense(x, training = training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x "
      ],
      "metadata": {
        "id": "H3r3GMi8hNp1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(vocab_size = vocab_size,\n",
        "                embedding_dim = embedding_dim,\n",
        "                rnn_units = rnn_units)"
      ],
      "metadata": {
        "id": "yTt5zfTvj2Mt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each character the model looks ip the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of next character"
      ],
      "metadata": {
        "id": "lo18mRPhkEJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the model \n",
        "At this point the problem can be treated as standard classification problem. Given the previous RNN state, and input this time step, predict the class of next character. "
      ],
      "metadata": {
        "id": "ORHNIk00lHUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True)"
      ],
      "metadata": {
        "id": "ZQF8rFrJlXO1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'Adam',\n",
        "              loss = loss)"
      ],
      "metadata": {
        "id": "WtDzd023mBc5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checkpoint callback \n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only = True\n",
        ")"
      ],
      "metadata": {
        "id": "1pcaCIZcmKqV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Execute the training "
      ],
      "metadata": {
        "id": "8pvjlT99mm0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset,\n",
        "                    epochs = 20,\n",
        "                    callbacks = [checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8iZjI2kms0u",
        "outputId": "58182722-bc0e-4751-9bd9-20cdc64429e6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 14s 55ms/step - loss: 1.7190\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.5575\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.4574\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.3872\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.3349\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.2889\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2475\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 1.2077\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.1679\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.1269\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.0847\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.0399\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9926\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.9415\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.8905\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.8387\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.7875\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7378\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.6939\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.6524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate Text \n",
        "Simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state during execution. \n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n",
        "\n",
        "The following makes a single step prediction:"
      ],
      "metadata": {
        "id": "fWZyJeuem55P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "dQ82bPXyqlB8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "bF-6rAvwrbSG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ],
      "metadata": {
        "id": "LFwiVVlAsMuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-vNQR-zr1eN",
        "outputId": "221f0252-9a46-438a-d07f-4ea4c7abea82"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "A goodly store, let have firm.\n",
            "\n",
            "ESCALUS:\n",
            "To my king, were there, young Patience.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "What, will our protector?\n",
            "\n",
            "LUCIO:\n",
            "\n",
            "ISABELLA:\n",
            "Hear me, my noble.\n",
            "\n",
            "PROSPERO:\n",
            "'Tis out a purpose;\n",
            "And he shall die, be glass; and he suntimes dail\n",
            "On cold fish'd for his own son in\n",
            "I would be blinding out; and, whistlent shriur\n",
            "In that we have forspecting when Thomas Mowbray,\n",
            "As I come fithen delicitors,\n",
            "and very wealth as by it and wed.\n",
            "\n",
            "Third Servingman:\n",
            "Where's Barnardine?\n",
            "\n",
            "Provost:\n",
            "A name, comfort all this in your roors.\n",
            "Make me true, fellow, let uh my feeble sorrow,\n",
            "Or is reportly help in 'alig basement--\n",
            "If you live, if Kent thou readem as you remiss;\n",
            "As to say to them, and disperse than\n",
            "Remember these new blood was well proclaimed\n",
            "Cut in the happy visits of the city;\n",
            "Whilst I, with thought it shall be so: it is a man that kill of a\n",
            "help in Corioli haveness,\n",
            "So cibil the hardly own to sleep;\n",
            "His heart convey me to the orderis\n",
            "Set down it.\n",
            "\n",
            "LEONTES:\n",
            "Hold,\n",
            "Throw nothing father, then, to \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.534169912338257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yceXA73jtl4L",
        "outputId": "9265270b-c65a-4d86-8bae-dddb15372660"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nHark, Parish Mercirivile,\\nThere shall not attare you know,\\nShe bids you less one foot overbown ignorant,\\nAnd not an ear nichtain; if I had rather\\nYet that have coof'd undiscovered.\\n\\nLUCENTIO:\\nYet one sire living, not a woman:\\nGentlewoman would again, as if we wisdom thence.\\n\\nJOHN OF GAUNT:\\nThe time shall never deserved it.\\n\\nCLARENCE:\\nWe think that Bolingbroke it was a but pinn'd.\\n\\nPRINCE:\\n\\nGLOUCESTER:\\nI know not what; I think not for me: if he so,\\nWe have forsworn to close this alliance.\\nYou suggest you, the king, profit you that Clarence,\\nWere justic time with the stepherds.\\n\\nCOMINIUS:\\nYou have finded me: ort\\nCyment, I think, for one sister Bearing, back\\nUnterpret'st this weak answer. Pray, again;\\nTell him, my lord, to-dwell with me to-day;\\nFor I have often here be sent to be\\nLook for the people's; and upon this hand of\\npities, and no rememby of your\\ndaughter to her husband's lawful king:\\nUnder your grace, keep you not like a king;\\nBe thou mock'st our own protector.\\n\\nLord Mayor:\\nMarr\"\n",
            " b\"ROMEO:\\nNot purge Enforch is fear'd and knaves or pleasure,\\nAfflict as most roared in our all asseech\\nWhilst all goth for our cheeks,\\nWith all the love this Edward like a show!\\nI little corse, and list smorts of nine;\\nSon, where is Kate, the king is here!\\n\\nKING RICHARD II:\\nA flourish, till eet.\\n\\nCAMILLO:\\nYou say it is not.\\n\\nFLORIZEL:\\nSir, I am more inclined to know\\nI am no fire to pass\\nThese dukes of the princes, his fair damned speaks,\\nMy darkness is as to him his head's.\\n\\nGLOUCESTER:\\nIf it, but be gone and venom with his mind\\nThat you have done to Romeo? thou dost fall upon thee;\\nEve you a quarter of the sweet, that know I\\ncannot tent watery against himself; it said\\n'The supers till the traitor of this blood:\\nThe francings are o' the recour conference,\\nOf what you are if first he did it to the sime\\nOn of an aired you on my knees,\\nThe very mercy of the earth to cambic in.\\n\\nANTONIO:\\nThe aim he hath too hand to the\\nsipplarent here comes Clarence' and I know before.\\n\\nJULIET:\\nThis tiger-disgrace\"\n",
            " b\"ROMEO:\\nThe words o' the toic; reprieve but they\\nThat Henry falls, and longs that ever Edward\\nSends you nor no less traitor.\\n\\nMosts:\\nYea, and therefore I'll uncle God, I can smile to any thing\\nin the age, her breastres nest of thee\\nShall love her to mis-swearing whencher sweetly. Their bed,\\nHe's beat afeard; and have no pupsit\\nBut in health possessain hath crosp'd!\\n\\nJULIET:\\nNo, now they'll quickly.\\n\\nCOMINIUS:\\nIs the news hold in virtue's girts?\\n\\nGREEN:\\nHere come the large me all my zemit.\\nWhate'er are my subjects?\\n\\nAUTOLYCUS:\\nHere's VolsciS:\\nSir, I shall bring you warrant.\\n\\nPETRUCHIO:\\nMistress Barnardine!\\n\\nPROSPERO:\\nBy what? ever fall our outward back o' the same and\\nrid me and her gurse is nothing it kills.\\nO, my fair cousin, you might shed for the\\nsingly.\\n\\nAUTOLY:\\nSpeak bratle; be accept of woman, the offence\\nTo instite of any unwilling's death,\\nTo have us knowledge that thou art others' noble anger!\\nFarewell: the ockning thumb, laid, as foolishly,\\nI charge thee not; say you say that I am n\"\n",
            " b\"ROMEO:\\n\\nISABELLLA:\\nIs there more toil that I was banish'd: for he's at justice\\nLike presenth to a house; besides and logs,\\ncome schoommasters of all willing the house: 'twixt your\\ngood power, yours wife; I mean ourselves,\\nLet venom now, afterward condition?\\n\\nQUEEN ELIZABETH:\\nWhat else? and I have heard, I pray thee, good\\nGives for confire and entertain out.\\n\\nPETRUCHIO:\\nWell, he may chance to London Clifford! thou not,\\nFor every knave you fellow of us; whilst\\nHeaven do him wrong, and canst no jotrect presently\\nBy Clarence' and to set me innocent as it\\nin the wrect of year. O, I crothching her,\\nThe one is that, we will be mistress in\\nSuch ridden storm'd by him small cherch and weeping to him.\\nDid, being spent some other affairs.\\nThy blessings slier?\\n\\nSecond Gentleman:\\nI think, or in remorse shall breed Aufidius\\nWith words indeed. If she had been a\\nbriefly, canst give liancas, time to say:\\nYou plain, my lord; leave but the freedoms of thine.\\n\\nJOve\\n\\nGREMIO:\\n\\nKATHARINA:\\nTo have done with him.\\n\\nDU\"\n",
            " b\"ROMEO:\\nBefore a purpos time is outran, lest you find\\nThese times of waters in this city be vows'd off.\\n\\nPETRUCHIO:\\nMarry, so gracious mine.\\n\\nCLIFFORD:\\nI not a cup of state, whose addice thou artined\\nWith a pursuad falling fellow:\\nIf he be tarded me to opeot her;\\nAnd now in happiness it true! or else\\nTo County Paris, now i' the let\\nBefore the swords of what is done to Rome,\\nLe, that thy father, do me this thirty\\nWas born another steed against the towns\\nOr both are businesses that are not unish'd,\\nThe father had dispersed the Aspiring shades\\nSome innocence; we will think it\\nThe lands and then in joy transported that\\nReven in their chafterable ways,\\nWhich 'tis in our moutn coming intortene\\nThat come to sacciand from destiny of her\\nThe bastard from the usurper's skulls,\\nand then am near itself; but if she be\\nIn that, to instructed nature?\\n\\nSecond Servingman:\\nWhere should be thus o' the ear blood\\nWhich here to chaste to us.\\n\\nVIRGILIA:\\nO, belike, you will! the king first been in perfect\\nIs quier c\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.856302976608276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Export the generator \n",
        "The single step can easily be saved and restored, allowing you to use it anywhere a `tf.saved_model` is accepted. "
      ],
      "metadata": {
        "id": "lHpbQMH6t7OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD0arubWuXqv",
        "outputId": "6b11664e-b765-45dd-fb40-1435f24ca986"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f549b44bb20>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7banivluig6",
        "outputId": "eeb99d22-c7fb-4827-c0c8-9c52db1e9bc7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "By my heart, a very traitor.\n",
            "\n",
            "Clown:\n",
            "I would I had some impusence; for\n",
            "the other for my thoughts ar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yaJmwhsJum3c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}