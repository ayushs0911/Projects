{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOhksk8Ffq7SdpAJcHboUDq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushs0911/Projects/blob/main/NLP/Drake_Lyrics_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem Statement \n",
        "**Character based RNN Model** <br>\n",
        "Text Generation model, which outputs Drake Style lyrics from any English Language inputs. \n",
        "\n",
        "Given a sequence of characters from the data, training a model to predict the next character in the sequence. Longer sequences of text can be generated by calling the model repeatedly.\n"
      ],
      "metadata": {
        "id": "B9sN5ur_jl0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading Datset"
      ],
      "metadata": {
        "id": "IVLqbCkQZ1kw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34SgJMkIvyuk",
        "outputId": "efda7616-fa67-4889-d133-d8854e7e3c7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Downloading drake-lyrics.zip to /content\n",
            "  0% 0.00/764k [00:00<?, ?B/s]\n",
            "100% 764k/764k [00:00<00:00, 112MB/s]\n",
            "Archive:  /content/drake-lyrics.zip\n",
            "  inflating: /content/dataset/drake_data.csv  \n",
            "  inflating: /content/dataset/drake_data.json  \n",
            "  inflating: /content/dataset/drake_lyrics.txt  \n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d juicobowley/drake-lyrics\n",
        "!unzip \"/content/drake-lyrics.zip\" -d \"/content/dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('/content/dataset/drake_lyrics.txt', 'rb').read().decode(encoding = 'utf-8')"
      ],
      "metadata": {
        "id": "mCtQOORxwHK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of text : {len(text)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_ElJx0AxBfq",
        "outputId": "c4f6882a-f4c7-494c-d96b-5d35ce58a556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text : 791643 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CnYDugKFxJmT",
        "outputId": "a53867ad-a632-4d21-a04b-d95b65336b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"[Verse]\\r\\nPut my feelings on ice\\r\\nAlways been a gem\\r\\nCertified lover boy, somehow still heartless\\r\\nH'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking unique characters in file"
      ],
      "metadata": {
        "id": "hNhbKZinxrp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKe77-vIxnnn",
        "outputId": "b29d9cfc-caa0-4c0f-8a6a-7bb3d00400f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Processing the text <br>\n",
        "`tf.keras.layers.StringLookup` layer can convert each character into a numberic ID. It just needs the text to be split into tokens first. "
      ],
      "metadata": {
        "id": "KvWVLibRxy4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import os \n",
        "import time \n",
        "     "
      ],
      "metadata": {
        "id": "fnnJifQBJPmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary = list(vocab),\n",
        "    mask_token = None\n",
        ")"
      ],
      "metadata": {
        "id": "z32aQeRhyWWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = [text[:100]]\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding = 'UTF-8')"
      ],
      "metadata": {
        "id": "b2dN0FBpylw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n4kit4hy1o9",
        "outputId": "b7057925-1a60-4906-b1bb-efe6b892219e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'\"', b'[', b'V', b'e', b'r', b's', b'e', b']', b'\\r', b'\\n', b'P',\n",
              "  b'u', b't', b' ', b'm', b'y', b' ', b'f', b'e', b'e', b'l', b'i', b'n',\n",
              "  b'g', b's', b' ', b'o', b'n', b' ', b'i', b'c', b'e', b'\\r', b'\\n',\n",
              "  b'A', b'l', b'w', b'a', b'y', b's', b' ', b'b', b'e', b'e', b'n', b' ',\n",
              "  b'a', b' ', b'g', b'e', b'm', b'\\r', b'\\n', b'C', b'e', b'r', b't',\n",
              "  b'i', b'f', b'i', b'e', b'd', b' ', b'l', b'o', b'v', b'e', b'r', b' ',\n",
              "  b'b', b'o', b'y', b',', b' ', b's', b'o', b'm', b'e', b'h', b'o', b'w',\n",
              "  b' ', b's', b't', b'i', b'l', b'l', b' ', b'h', b'e', b'a', b'r', b't',\n",
              "  b'l', b'e', b's', b's', b'\\r', b'\\n', b'H']]>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)"
      ],
      "metadata": {
        "id": "e_JcSwdUzTP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BYJw83ezXCo",
        "outputId": "4da7eea1-5d76-4b1e-977e-46d8b2dc466b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[5, 57, 52, 64, 77, 78, 64, 58, 2, 1, 46, 80, 79, 3, 72, 84, 3, 65, 64,\n",
              "  64, 71, 68, 73, 66, 78, 3, 74, 73, 3, 68, 62, 64, 2, 1, 31, 71, 82, 60,\n",
              "  84, 78, 3, 61, 64, 64, 73, 3, 60, 3, 66, 64, 72, 2, 1, 33, 64, 77, 79,\n",
              "  68, 65, 68, 64, 63, 3, 71, 74, 81, 64, 77, 3, 61, 74, 84, 14, 3, 78, 74,\n",
              "  72, 64, 67, 74, 82, 3, 78, 79, 68, 71, 71, 3, 67, 64, 60, 77, 79, 71,\n",
              "  64, 78, 78, 2, 1, 38]]>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inverting this representation and recover human readable strings from it. "
      ],
      "metadata": {
        "id": "GMPjlYADzbma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary = ids_from_chars.get_vocabulary(),\n",
        "    invert = True,\n",
        "    mask_token = None\n",
        ")"
      ],
      "metadata": {
        "id": "K0wDlvlazYky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars\n",
        "tf.strings.reduce_join(chars, axis = -1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35PsdbkVzvjs",
        "outputId": "c91e00f2-38ba-44f3-acff-a5db750fe512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'\"[Verse]\\r\\nPut my feelings on ice\\r\\nAlways been a gem\\r\\nCertified lover boy, somehow still heartless\\r\\nH'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis =-1)"
      ],
      "metadata": {
        "id": "TcuaSw_D2q6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The prediction task\n",
        "\n",
        "The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
        "\n",
        "**Create training examples and targets**\n",
        "\n",
        "- Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "- For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "- So break the text into chunks of `seq_length+1`. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ],
      "metadata": {
        "id": "1-ffgtV_0HFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjzpjR_czyT7",
        "outputId": "077e47a9-40fd-4c0f-90d9-2ad302e005d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(791643,), dtype=int64, numpy=array([ 5, 57, 52, ...,  5,  2,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n"
      ],
      "metadata": {
        "id": "Swn1fUgu0bVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(20):\n",
        "  print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9XU3Dza1NqO",
        "outputId": "048131db-d75f-4c8d-fe22-981c0ce79b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\n",
            "[\n",
            "V\n",
            "e\n",
            "r\n",
            "s\n",
            "e\n",
            "]\n",
            "\r\n",
            "\n",
            "\n",
            "P\n",
            "u\n",
            "t\n",
            " \n",
            "m\n",
            "y\n",
            " \n",
            "f\n",
            "e\n",
            "e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "oHTPu-6L1X-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder = True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(seq)\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hZnr7Xy1eGi",
        "outputId": "85e639e7-07d2-477a-c4a2-0f17b098f8ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[ 5 57 52 64 77 78 64 58  2  1 46 80 79  3 72 84  3 65 64 64 71 68 73 66\n",
            " 78  3 74 73  3 68 62 64  2  1 31 71 82 60 84 78  3 61 64 64 73  3 60  3\n",
            " 66 64 72  2  1 33 64 77 79 68 65 68 64 63  3 71 74 81 64 77  3 61 74 84\n",
            " 14  3 78 74 72 64 67 74 82  3 78 79 68 71 71  3 67 64 60 77 79 71 64 78\n",
            " 78  2  1 38 64], shape=(101,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[b'\"' b'[' b'V' b'e' b'r' b's' b'e' b']' b'\\r' b'\\n' b'P' b'u' b't' b' '\n",
            " b'm' b'y' b' ' b'f' b'e' b'e' b'l' b'i' b'n' b'g' b's' b' ' b'o' b'n'\n",
            " b' ' b'i' b'c' b'e' b'\\r' b'\\n' b'A' b'l' b'w' b'a' b'y' b's' b' ' b'b'\n",
            " b'e' b'e' b'n' b' ' b'a' b' ' b'g' b'e' b'm' b'\\r' b'\\n' b'C' b'e' b'r'\n",
            " b't' b'i' b'f' b'i' b'e' b'd' b' ' b'l' b'o' b'v' b'e' b'r' b' ' b'b'\n",
            " b'o' b'y' b',' b' ' b's' b'o' b'm' b'e' b'h' b'o' b'w' b' ' b's' b't'\n",
            " b'i' b'l' b'l' b' ' b'h' b'e' b'a' b'r' b't' b'l' b'e' b's' b's' b'\\r'\n",
            " b'\\n' b'H' b'e'], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4Y79onZ118K",
        "outputId": "aca4e8ca-c7e4-421a-f6eb-7a0674d8c95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'\"[Verse]\\r\\nPut my feelings on ice\\r\\nAlways been a gem\\r\\nCertified lover boy, somehow still heartless\\r\\nHe'\n",
            "b'art is only gettin\\' colder\"\\r\\n\"[Verse]\\r\\nHands are tied\\r\\nSomeone\\'s in my ear from the other side\\r\\nTelli'\n",
            "b\"n' me that I should pay you no mind\\r\\nWanted you to not be with me all night\\r\\nWanted you to not stay w\"\n",
            "b\"ith me all night\\r\\nI know, you know, who that person is to me\\r\\nDoesn't really change things\\r\\n\\r\\n[Chorus\"\n",
            "b\"]\\r\\nI know you're scared of dating, falling for me\\r\\nShorty, surely you know me\\r\\nRight here for you alw\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "rgpOt9pc2I67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input, target = split_input_target(list('Ayush Singh'))\n",
        "input, target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kooEHVy3PSv",
        "outputId": "a3aec039-3e5e-4726-a7ee-f23dc8984ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['A', 'y', 'u', 's', 'h', ' ', 'S', 'i', 'n', 'g'],\n",
              " ['y', 'u', 's', 'h', ' ', 'S', 'i', 'n', 'g', 'h'])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "TJzgiIwd3XaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input:\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmx3BizQ4KHV",
        "outputId": "7a5d495d-a3d5-4012-fa06-9c16299d3c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: b'\"[Verse]\\r\\nPut my feelings on ice\\r\\nAlways been a gem\\r\\nCertified lover boy, somehow still heartless\\r\\nH'\n",
            "Target b'[Verse]\\r\\nPut my feelings on ice\\r\\nAlways been a gem\\r\\nCertified lover boy, somehow still heartless\\r\\nHe'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating training batches \n",
        "Shuffle the data and pack into batches "
      ],
      "metadata": {
        "id": "YcJrvdhO4fg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = (dataset\n",
        "           .shuffle(BUFFER_SIZE)\n",
        "           .batch(BATCH_SIZE, drop_remainder = True)\n",
        "           .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHeySYHQ4whj",
        "outputId": "e9d43739-a0ed-43a3-d677-a5aca88a9237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build the model \n",
        "Defines model as `keras.Model` subclass. \n",
        "3 layers \n",
        "- `tf.keras.layers.Embedding`\n",
        "- `tf.keras.layers.GRU`\n",
        "- `tf.keras.layers.Dense` : output layer with `vocab_size` outputs. It outputs one logit for each character in vocabulary. "
      ],
      "metadata": {
        "id": "AAJcaDmI5ICO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#length of vocab in StringLookup layer \n",
        "\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "jDWOlxQu5xBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, \n",
        "                                   return_sequences = True, \n",
        "                                   return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training = training )\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state = states, \n",
        "                         training = training)\n",
        "    x = self.dense(x, training = training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x "
      ],
      "metadata": {
        "id": "DAmZhjT06WHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(vocab_size = vocab_size,\n",
        "                embedding_dim = embedding_dim,\n",
        "                rnn_units = rnn_units)"
      ],
      "metadata": {
        "id": "sb6DfPXhBr0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the model \n",
        "At this point the problem can be treated as standard classification problem. Given the previous RNN Statet and input this time step, predict the class of next character. "
      ],
      "metadata": {
        "id": "6Qweug-LCA87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True)"
      ],
      "metadata": {
        "id": "y1qFxz3cCUXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'Adam', \n",
        "              loss = loss)\n"
      ],
      "metadata": {
        "id": "yqGqYghICa-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checkpoint callback \n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only = True\n",
        ")"
      ],
      "metadata": {
        "id": "x4I7D_tNCgH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Execute the training "
      ],
      "metadata": {
        "id": "6HAKQvNOCx1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs = 30,\n",
        "                    callbacks = [checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gktIEwIChwe",
        "outputId": "e163fa76-d363-4db8-fbfd-afb02a1f8465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "122/122 [==============================] - 13s 64ms/step - loss: 2.8560\n",
            "Epoch 2/30\n",
            "122/122 [==============================] - 8s 57ms/step - loss: 2.0426\n",
            "Epoch 3/30\n",
            "122/122 [==============================] - 9s 58ms/step - loss: 1.7399\n",
            "Epoch 4/30\n",
            "122/122 [==============================] - 8s 56ms/step - loss: 1.5572\n",
            "Epoch 5/30\n",
            "122/122 [==============================] - 8s 56ms/step - loss: 1.4331\n",
            "Epoch 6/30\n",
            "122/122 [==============================] - 8s 55ms/step - loss: 1.3412\n",
            "Epoch 7/30\n",
            "122/122 [==============================] - 8s 55ms/step - loss: 1.2625\n",
            "Epoch 8/30\n",
            "122/122 [==============================] - 8s 55ms/step - loss: 1.1941\n",
            "Epoch 9/30\n",
            "122/122 [==============================] - 8s 55ms/step - loss: 1.1285\n",
            "Epoch 10/30\n",
            "122/122 [==============================] - 9s 56ms/step - loss: 1.0649\n",
            "Epoch 11/30\n",
            "122/122 [==============================] - 8s 56ms/step - loss: 0.9992\n",
            "Epoch 12/30\n",
            "122/122 [==============================] - 8s 57ms/step - loss: 0.9336\n",
            "Epoch 13/30\n",
            "122/122 [==============================] - 9s 56ms/step - loss: 0.8644\n",
            "Epoch 14/30\n",
            "122/122 [==============================] - 8s 56ms/step - loss: 0.7947\n",
            "Epoch 15/30\n",
            "122/122 [==============================] - 8s 56ms/step - loss: 0.7245\n",
            "Epoch 16/30\n",
            "122/122 [==============================] - 9s 56ms/step - loss: 0.6556\n",
            "Epoch 17/30\n",
            "122/122 [==============================] - 9s 56ms/step - loss: 0.5893\n",
            "Epoch 18/30\n",
            "122/122 [==============================] - 8s 57ms/step - loss: 0.5282\n",
            "Epoch 19/30\n",
            "122/122 [==============================] - 8s 55ms/step - loss: 0.4732\n",
            "Epoch 20/30\n",
            "122/122 [==============================] - 9s 56ms/step - loss: 0.4259\n",
            "Epoch 21/30\n",
            "122/122 [==============================] - 8s 56ms/step - loss: 0.3848\n",
            "Epoch 22/30\n",
            "122/122 [==============================] - 8s 56ms/step - loss: 0.3514\n",
            "Epoch 23/30\n",
            "122/122 [==============================] - 8s 55ms/step - loss: 0.3213\n",
            "Epoch 24/30\n",
            "122/122 [==============================] - 9s 56ms/step - loss: 0.3008\n",
            "Epoch 25/30\n",
            "122/122 [==============================] - 8s 55ms/step - loss: 0.2841\n",
            "Epoch 26/30\n",
            "122/122 [==============================] - 8s 57ms/step - loss: 0.2679\n",
            "Epoch 27/30\n",
            "122/122 [==============================] - 8s 56ms/step - loss: 0.2548\n",
            "Epoch 28/30\n",
            "122/122 [==============================] - 9s 56ms/step - loss: 0.2468\n",
            "Epoch 29/30\n",
            "122/122 [==============================] - 8s 56ms/step - loss: 0.2394\n",
            "Epoch 30/30\n",
            "122/122 [==============================] - 8s 56ms/step - loss: 0.2352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate Text\n",
        "Simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state during execution. \n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text. \n",
        "The following makes a single step prediction :"
      ],
      "metadata": {
        "id": "8fHJoif9JUky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    print(sparse_mask.shape)\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "    \n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    print(f\"1: {predicted_logits.shape}\")\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "    print(predicted_logits.shape)\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "     "
      ],
      "metadata": {
        "id": "ZJixXWOpL2ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOH_t6WiPLw3",
        "outputId": "82fc86d9-6a9c-44d1-d414-4df9aae8d1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(106,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"Yeah, Usher\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe0Bc9EXPNfI",
        "outputId": "ee439619-6b15-49b9-d8d9-a9ce2a5666f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yeah, Usher. Senthereal the paperwation is that 8 2n law as the same spirt\r\n",
            "'Cause I got real hitters over new it, reluations\r\n",
            "I was 2 ma, with J somethin', state up to me\r\n",
            "Everything that I write is either for y'all to getting close\r\n",
            "I tried to tell me \"\"Get it\"\"\r\n",
            "Then is this women that I know we here\r\n",
            "I'm not acting up, you can find it like that\r\n",
            "Only you can do it real girl, wait till the time\r\n",
            "Timeling is comin' back around and get ya\r\n",
            "It's the ones that you're explaining for me\r\n",
            "Like I'm Louing bag, is crossed, that what I might\r\n",
            "First turned up, I let that shit sind it, I'm in love with easones\r\n",
            "I don't wanna seem like four words\r\n",
            "This mother spoken like Kala Pashos the past then I heard it\r\n",
            "Niggas know what I'm sayin'?\r\n",
            "\r\n",
            "[Outro: Majid Al Massai]\r\n",
            "'amony\"\"\r\n",
            "And welcome to my power\r\n",
            "I remember when my life every nigga in the tankin'\r\n",
            "Cun a Grammy like a motherfucking crass\r\n",
            "Look, what you call, you not the belate\r\n",
            "Man, she's not the honest, they know it's becked up the very face\r\n",
            "I don't d \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.196868419647217\n"
          ]
        }
      ]
    }
  ]
}